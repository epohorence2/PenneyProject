Both of us chose different methods for storing data; one focusing on seed-based deck reconstruction for immediate scoring, and the other method pursuing bulk deck generation. While both methods accomplish reproducibility and generation of two million decks available for the Penney's and Humble-Nishiyama Randomness games, basic benchmark results reveal a general preference towards seed-based regeneration. 
The direct bulk storage uses twenty batches of 100,000 decks to avoid GitHub file constraints alongside a seed file for reproducibility. The process involved generating a binary array (2,000,000, 52) with a random number generator and permutation to encourage independent shuffling. After array generation, the twenty batches are saved in a separate data file. The seed-based reconstruction !!! 
We compared our results using the same tests with the assistance of different debuggers, then compared the results with a shared debugger to compare capabilities. After multiple runs, the memory usage and average runtime favors the seed-based pipeline. Revealed as the most notable difference through the benchmark results, bulk deck generation consumes over fifty times the memory usage than the seed-based decks. Alongside memory usage, the overall seed-based generation runs four times faster than the bulk generation; however, the read time is nominally smaller for the bulk method. 